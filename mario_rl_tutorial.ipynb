{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Mario-playing RL Agent\n",
    "==============================\n",
    "\n",
    "**Authors:** [Yuansong Feng](https://github.com/YuansongFeng), [Suraj\n",
    "Subramanian](https://github.com/suraj813), [Howard\n",
    "Wang](https://github.com/hw26), [Steven\n",
    "Guo](https://github.com/GuoYuzhang).\n",
    "\n",
    "This tutorial walks you through the fundamentals of Deep Reinforcement\n",
    "Learning. At the end, you will implement an AI-powered Mario (using\n",
    "[Double Deep Q-Networks](https://arxiv.org/pdf/1509.06461.pdf)) that can\n",
    "play the game by itself.\n",
    "\n",
    "Although no prior knowledge of RL is necessary for this tutorial, you\n",
    "can familiarize yourself with these RL\n",
    "[concepts](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html),\n",
    "and have this handy\n",
    "[cheatsheet](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N)\n",
    "as your companion. The full code is available\n",
    "[here](https://github.com/yuansongFeng/MadMario/).\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/mario.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym-super-mario-bros==7.4.0\n",
    "# !pip install tensordict==0.3.0\n",
    "# !pip install torchrl==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL Definitions\n",
    "==============\n",
    "\n",
    "**Environment** The world that an agent interacts with and learns from.\n",
    "\n",
    "**Action** $a$ : How the Agent responds to the Environment. The set of\n",
    "all possible Actions is called *action-space*.\n",
    "\n",
    "**State** $s$ : The current characteristic of the Environment. The set\n",
    "of all possible States the Environment can be in is called\n",
    "*state-space*.\n",
    "\n",
    "**Reward** $r$ : Reward is the key feedback from Environment to Agent.\n",
    "It is what drives the Agent to learn and to change its future action. An\n",
    "aggregation of rewards over multiple time steps is called **Return**.\n",
    "\n",
    "**Optimal Action-Value function** $Q^*(s,a)$ : Gives the expected return\n",
    "if you start in state $s$, take an arbitrary action $a$, and then for\n",
    "each future time step take the action that maximizes returns. $Q$ can be\n",
    "said to stand for the \"quality\" of the action in a state. We try to\n",
    "approximate this function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment\n",
    "===========\n",
    "\n",
    "Initialize Environment\n",
    "----------------------\n",
    "\n",
    "In Mario, the environment consists of tubes, mushrooms and other\n",
    "components.\n",
    "\n",
    "When Mario makes an action, the environment responds with the changed\n",
    "(next) state, reward and other info.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    }
   ],
   "source": [
    "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n",
    "if gym.__version__ < '0.26':\n",
    "    # env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\", new_step_api=True)\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\")\n",
    "else:\n",
    "    # env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\", render_mode='rgb', apply_api_compatibility=True)\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\", render_mode='rgb')\n",
    "\n",
    "# Limit the action-space to\n",
    "#   0. walk right\n",
    "#   1. jump right\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "env.reset()\n",
    "# next_state, reward, done, trunc, info = env.step(action=0)\n",
    "next_state, reward, done, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Environment\n",
    "======================\n",
    "\n",
    "Environment data is returned to the agent in `next_state`. As you saw\n",
    "above, each state is represented by a `[3, 240, 256]` size array. Often\n",
    "that is more information than our agent needs; for instance, Mario's\n",
    "actions do not depend on the color of the pipes or the sky!\n",
    "\n",
    "We use **Wrappers** to preprocess environment data before sending it to\n",
    "the agent.\n",
    "\n",
    "`GrayScaleObservation` is a common wrapper to transform an RGB image to\n",
    "grayscale; doing so reduces the size of the state representation without\n",
    "losing useful information. Now the size of each state: `[1, 240, 256]`\n",
    "\n",
    "`ResizeObservation` downsamples each observation into a square image.\n",
    "New size: `[1, 84, 84]`\n",
    "\n",
    "`SkipFrame` is a custom wrapper that inherits from `gym.Wrapper` and\n",
    "implements the `step()` function. Because consecutive frames don't vary\n",
    "much, we can skip n-intermediate frames without losing much information.\n",
    "The n-th frame aggregates rewards accumulated over each skipped frame.\n",
    "\n",
    "`FrameStack` is a wrapper that allows us to squash consecutive frames of\n",
    "the environment into a single observation point to feed to our learning\n",
    "model. This way, we can identify if Mario was landing or jumping based\n",
    "on the direction of his movement in the previous several frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            # obs, reward, done, trunk, info = self.env.step(action)\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "if gym.__version__ < '0.26':\n",
    "    # env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the above wrappers to the environment, the final wrapped\n",
    "state consists of 4 gray-scaled consecutive frames stacked together, as\n",
    "shown above in the image on the left. Each time Mario makes an action,\n",
    "the environment responds with a state of this structure. The structure\n",
    "is represented by a 3-D array of size `[4, 84, 84]`.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/mario_env.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent\n",
    "=====\n",
    "\n",
    "We create a class `Mario` to represent our agent in the game. Mario\n",
    "should be able to:\n",
    "\n",
    "-   **Act** according to the optimal action policy based on the current\n",
    "    state (of the environment).\n",
    "-   **Remember** experiences. Experience = (current state, current\n",
    "    action, reward, next state). Mario *caches* and later *recalls* his\n",
    "    experiences to update his action policy.\n",
    "-   **Learn** a better action policy over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
    "        pass\n",
    "\n",
    "    def cache(self, experience):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"Sample experiences from memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we will populate Mario's parameters and\n",
    "define his functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Act\n",
    "===\n",
    "\n",
    "For any given state, an agent can choose to do the most optimal action\n",
    "(**exploit**) or a random action (**explore**).\n",
    "\n",
    "Mario randomly explores with a chance of `self.exploration_rate`; when\n",
    "he chooses to exploit, he relies on `MarioNet` (implemented in `Learn`\n",
    "section) to provide the most optimal action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        self.net = self.net.to(device=self.device)\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "    Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "    Inputs:\n",
    "    state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n",
    "    Outputs:\n",
    "    ``action_idx`` (``int``): An integer representing which action Mario will perform\n",
    "    \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache and Recall\n",
    "================\n",
    "\n",
    "These two functions serve as Mario's \"memory\" process.\n",
    "\n",
    "`cache()`: Each time Mario performs an action, he stores the\n",
    "`experience` to his memory. His experience includes the current *state*,\n",
    "*action* performed, *reward* from the action, the *next state*, and\n",
    "whether the game is *done*.\n",
    "\n",
    "`recall()`: Mario randomly samples a batch of experiences from his\n",
    "memory, and uses that to learn the game.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):  # subclassing for continuity\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (``LazyFrame``),\n",
    "        next_state (``LazyFrame``),\n",
    "        action (``int``),\n",
    "        reward (``float``),\n",
    "        done(``bool``))\n",
    "        \"\"\"\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "        state = torch.tensor(state)\n",
    "        next_state = torch.tensor(next_state)\n",
    "        action = torch.tensor([action])\n",
    "        reward = torch.tensor([reward])\n",
    "        done = torch.tensor([done])\n",
    "\n",
    "        # self.memory.append((state, next_state, action, reward, done,))\n",
    "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
    "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn\n",
    "=====\n",
    "\n",
    "Mario uses the [DDQN algorithm](https://arxiv.org/pdf/1509.06461) under\n",
    "the hood. DDQN uses two ConvNets - $Q_{online}$ and $Q_{target}$ - that\n",
    "independently approximate the optimal action-value function.\n",
    "\n",
    "In our implementation, we share feature generator `features` across\n",
    "$Q_{online}$ and $Q_{target}$, but maintain separate FC classifiers for\n",
    "each. $\\theta_{target}$ (the parameters of $Q_{target}$) is frozen to\n",
    "prevent updating by backprop. Instead, it is periodically synced with\n",
    "$\\theta_{online}$ (more on this later).\n",
    "\n",
    "Neural Network\n",
    "--------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    \"\"\"mini CNN structure\n",
    "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = self.__build_cnn(c, output_dim)\n",
    "\n",
    "        self.target = self.__build_cnn(c, output_dim)\n",
    "        self.target.load_state_dict(self.online.state_dict())\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "\n",
    "    def __build_cnn(self, c, output_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD Estimate & TD Target\n",
    "=======================\n",
    "\n",
    "Two values are involved in learning:\n",
    "\n",
    "**TD Estimate** - the predicted optimal $Q^*$ for a given state $s$\n",
    "\n",
    "$${TD}_e = Q_{online}^*(s,a)$$\n",
    "\n",
    "**TD Target** - aggregation of current reward and the estimated $Q^*$ in\n",
    "the next state $s'$\n",
    "\n",
    "$$a' = argmax_{a} Q_{online}(s', a)$$\n",
    "\n",
    "$${TD}_t = r + \\gamma Q_{target}^*(s',a')$$\n",
    "\n",
    "Because we don't know what next action $a'$ will be, we use the action\n",
    "$a'$ maximizes $Q_{online}$ in the next state $s'$.\n",
    "\n",
    "Notice we use the\n",
    "[\\@torch.no\\_grad()](https://pytorch.org/docs/stable/generated/torch.no_grad.html#no-grad)\n",
    "decorator on `td_target()` to disable gradient calculations here\n",
    "(because we don't need to backpropagate on $\\theta_{target}$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the model\n",
    "==================\n",
    "\n",
    "As Mario samples inputs from his replay buffer, we compute $TD_t$ and\n",
    "$TD_e$ and backpropagate this loss down $Q_{online}$ to update its\n",
    "parameters $\\theta_{online}$ ($\\alpha$ is the learning rate `lr` passed\n",
    "to the `optimizer`)\n",
    "\n",
    "$$\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)$$\n",
    "\n",
    "$\\theta_{target}$ does not update through backpropagation. Instead, we\n",
    "periodically copy $\\theta_{online}$ to $\\theta_{target}$\n",
    "\n",
    "$$\\theta_{target} \\leftarrow \\theta_{online}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save checkpoint\n",
    "===============\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together\n",
    "=======================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging\n",
    "=======\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
    "            plt.clf()\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
    "            plt.legend()\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play!\n",
    "===========\n",
    "\n",
    "In this example we run the training loop for 40 episodes, but for Mario\n",
    "to truly learn the ways of his world, we suggest running the loop for at\n",
    "least 40,000 episodes!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Episode 0 - Step 48 - Epsilon 0.9999880000704982 - Mean Reward 232.0 - Mean Length 48.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.443 - Time 2024-05-04T13:50:49\n",
      "Episode 20 - Step 9443 - Epsilon 0.9976420340840456 - Mean Reward 891.714 - Mean Length 449.667 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 68.897 - Time 2024-05-04T13:51:58\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret 'torch.int64' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m mario\u001b[38;5;241m.\u001b[39mcache(state, next_state, action, reward, done)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Learn\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m q, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmario\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Logging\u001b[39;00m\n\u001b[0;32m     37\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog_step(reward, loss, q)\n",
      "Cell \u001b[1;32mIn[13], line 22\u001b[0m, in \u001b[0;36mMario.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Sample from memory\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m state, next_state, action, reward, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Get TD Estimate\u001b[39;00m\n\u001b[0;32m     25\u001b[0m td_est \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtd_estimate(state, action)\n",
      "Cell \u001b[1;32mIn[8], line 36\u001b[0m, in \u001b[0;36mMario.recall\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecall\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m    Retrieve a batch of experiences from memory\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     37\u001b[0m     state, next_state, action, reward, done \u001b[38;5;241m=\u001b[39m (batch\u001b[38;5;241m.\u001b[39mget(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_state\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state, next_state, action\u001b[38;5;241m.\u001b[39msqueeze(), reward\u001b[38;5;241m.\u001b[39msqueeze(), done\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bmrl\\lib\\site-packages\\torchrl\\data\\replay_buffers\\replay_buffers.py:964\u001b[0m, in \u001b[0;36mTensorDictReplayBuffer.sample\u001b[1;34m(self, batch_size, return_info, include_info)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    957\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_info is going to be deprecated soon.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe default behaviour has changed to `include_info=True` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto avoid bugs linked to wrongly preassigned values in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    961\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput tensordict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m data, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    965\u001b[0m is_tc \u001b[38;5;241m=\u001b[39m is_tensor_collection(data)\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tc \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensorclass(data) \u001b[38;5;129;01mand\u001b[39;00m include_info \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bmrl\\lib\\site-packages\\torchrl\\data\\replay_buffers\\replay_buffers.py:520\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[1;34m(self, batch_size, return_info)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size not specified. You can specify the batch_size when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstructing the replay buffer, or pass it to the sample method. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRefer to the ReplayBuffer documentation \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor a proper usage of the batch-size arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    518\u001b[0m     )\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch:\n\u001b[1;32m--> 520\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_futures_lock:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bmrl\\lib\\site-packages\\torchrl\\data\\replay_buffers\\utils.py:49\u001b[0m, in \u001b[0;36mpin_memory_output.<locals>.decorated_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorated_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 49\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m     51\u001b[0m         _tuple_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bmrl\\lib\\site-packages\\torchrl\\data\\replay_buffers\\replay_buffers.py:459\u001b[0m, in \u001b[0;36mReplayBuffer._sample\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    457\u001b[0m     index, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage, batch_size)\n\u001b[0;32m    458\u001b[0m     info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[1;32m--> 459\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, INT_CLASSES):\n\u001b[0;32m    461\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collate_fn(data)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bmrl\\lib\\site-packages\\torchrl\\data\\replay_buffers\\storages.py:886\u001b[0m, in \u001b[0;36mLazyMemmapStorage.get\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: Union[\u001b[38;5;28mint\u001b[39m, Sequence[\u001b[38;5;28mint\u001b[39m], \u001b[38;5;28mslice\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 886\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;66;03m# to be deprecated in v0.4\u001b[39;00m\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_device\u001b[39m(tensor):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bmrl\\lib\\site-packages\\torchrl\\data\\replay_buffers\\storages.py:606\u001b[0m, in \u001b[0;36mTensorStorage.get\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get an item from an unitialized LazyMemmapStorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    604\u001b[0m     )\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tc:\n\u001b[1;32m--> 606\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reset_batch_size(out)\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bmrl\\lib\\site-packages\\tensordict\\base.py:264\u001b[0m, in \u001b[0;36mTensorDictBase.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m index):\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_tensordict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bmrl\\lib\\site-packages\\tensordict\\_td.py:777\u001b[0m, in \u001b[0;36mTensorDict._index_tensordict\u001b[1;34m(self, index, new_batch_size, names)\u001b[0m\n\u001b[0;32m    775\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m new_batch_size\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 777\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[43m_getitem_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    778\u001b[0m source \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bmrl\\lib\\site-packages\\tensordict\\utils.py:1646\u001b[0m, in \u001b[0;36m_getitem_batch_size\u001b[1;34m(batch_size, index)\u001b[0m\n\u001b[0;32m   1644\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(idx)\n\u001b[0;32m   1645\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, (torch\u001b[38;5;241m.\u001b[39mTensor, np\u001b[38;5;241m.\u001b[39mndarray)):\n\u001b[1;32m-> 1646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbool \u001b[38;5;129;01mor\u001b[39;00m \u001b[43midx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbool\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1647\u001b[0m         shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize([idx\u001b[38;5;241m.\u001b[39msum()])\n\u001b[0;32m   1648\u001b[0m         boolean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret 'torch.int64' as a data type"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use_cuda = torch.cuda.is_available()\n",
    "# print(f\"Using CUDA: {use_cuda}\")\n",
    "# print()\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 40\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        # next_state, reward, done, trunc, info = env.step(action)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if (e % 20 == 0) or (e == episodes - 1):\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'steps_done' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m state \u001b[38;5;241m=\u001b[39m current_screen \u001b[38;5;241m-\u001b[39m last_screen\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):  \u001b[38;5;66;03m# Don't infinite loop while learning\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Select and perform an action\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# action = select_action(stat)\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     action \u001b[38;5;241m=\u001b[39m select_action(state, \u001b[43msteps_done\u001b[49m, EPS_END, EPS_START, EPS_DECAY)\n\u001b[0;32m     34\u001b[0m     _, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     35\u001b[0m     reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([reward], device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'steps_done' is not defined"
     ]
    }
   ],
   "source": [
    "def get_screen(env):\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))  # transpose into torch order (CHW)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    return resize(screen).unsqueeze(0).to(device)  # add a batch dimension (BCHW)\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.BILINEAR),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def select_action(state, steps_done, EPS_END, EPS_START, EPS_DECAY):\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen(env)\n",
    "    current_screen = get_screen(env)\n",
    "    state = current_screen - last_screen\n",
    "    for t in range(100):  # Don't infinite loop while learning\n",
    "        # Select and perform an action\n",
    "        # action = select_action(stat)\n",
    "        action = select_action(state, steps_done, EPS_END, EPS_START, EPS_DECAY)\n",
    "\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen(env)\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()  # Add this line to plot the duration of each episode\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
